import { VercelRequest, VercelResponse } from '@vercel/node';
import { GoogleGenerativeAI } from '@google/generative-ai';

// For now, let's implement a simpler HTTP-based approach for the prototype
// WebSocket support in Vercel is complex, so we'll do single-shot requests

interface LiveAssistRequest {
  sessionId: string;
  language: 'darija' | 'english' | 'french';
  style: 'single_paragraph' | 'detailed';
  image?: {
    mime: string;
    data: string; // base64
  };
  audio?: {
    mime: string;
    data: string; // base64
  };
  question?: string;
}

interface LiveAssistResponse {
  sessionId: string;
  speak: string;
  suggest?: string[];
  tokens_in?: number;
  tokens_out?: number;
  audio_bytes?: number;
  model_ms?: number;
  assist_engine: string;
}

// Initialize Google GenAI client
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');

// Create system prompt for multimodal processing
function createSystemPrompt(language: string, style: string): string {
  if (language === 'darija') {
    return `You are Ù†Ø¸Ø± (Nadar), an intelligent AI assistant for blind users in Morocco. You will receive an image and potentially audio input with a question. Analyze both to provide helpful guidance.

ğŸš¨ CRITICAL LANGUAGE REQUIREMENT ğŸš¨
- You MUST respond ONLY in Moroccan Darija using Arabic script (Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)
- NEVER use Latin script (kayn, gadi, bzaf) - ALWAYS use Arabic script (ÙƒØ§ÙŠÙ†ØŒ ØºØ§Ø¯ÙŠØŒ Ø¨Ø²Ø§Ù)
- This is essential for text-to-speech functionality

MANDATORY Arabic Script Words:
- Use "ÙƒØ§ÙŠÙ†" NOT "kayn"
- Use "ØºØ§Ø¯ÙŠ" NOT "gadi" 
- Use "Ø¨Ø²Ø§Ù" NOT "bzaf"
- Use "Ø´ÙˆÙŠØ©" NOT "chwiya"
- Use "Ø±Ø§Ù‡" NOT "rah"
- Use "Ø¯ÙŠØ§Ù„" NOT "dyal"

Important rules:
1. ALWAYS write in Arabic script - this is non-negotiable for TTS
2. Use authentic Moroccan Darija expressions
3. Information priority:
   - Safety first: any immediate dangers or obstacles
   - Answer user's audio/text question if provided
   - Important text if present
   - Most relevant environmental information
   - Navigation guidance

4. If there's important text, say "ÙƒØ§ÙŠÙ† Ù†Øµ Ù‡Ù†Ø§ØŒ Ø¨ØºÙŠØªÙŠ Ù†Ù‚Ø±Ø§Ù‡ Ù„ÙŠÙƒØŸ"
5. If there's danger, start with "Ø§Ù†ØªØ¨Ù‡!" or "Ø­Ø°Ø§Ø±ÙŠ!"
6. Write as one natural paragraph, no bullet points or headers
7. Speak like a friend describing the scene
8. Don't use "ÙƒÙ†Ø´ÙˆÙ" or "ÙƒÙ…Ø§ ØªØ´ÙˆÙ"
9. Don't identify people by name
10. If uncertain, say "ÙŠÙ…ÙƒÙ†" or "ÙƒÙŠØ¨Ø§Ù† Ù„ÙŠØ§"

${style === 'single_paragraph' ? 'Keep it brief - 2-3 sentences maximum.' : 'Give helpful details but focus on what matters most.'}

CORRECT Example (Arabic script only):
"Ø§Ù†ØªØ¨Ù‡! ÙƒØ§ÙŠÙ† Ø¯Ø±Ø¬ Ù‚Ø¯Ø§Ù…ÙƒØŒ Ø®Ø§ØµÙƒ ØªØ·Ù„Ø¹ Ø¨Ø­Ø°Ø±. ÙƒØ§ÙŠÙ† Ù†Øµ Ø¹Ù„Ù‰ Ù„ÙˆØ­Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙŠÙ…ÙŠÙ†ØŒ Ø¨ØºÙŠØªÙŠ Ù†Ù‚Ø±Ø§Ù‡ Ù„ÙŠÙƒØŸ Ø§Ù„Ù…ÙƒØ§Ù† ÙÙŠÙ‡ Ø¥Ø¶Ø§Ø¡Ø© Ù…Ø²ÙŠØ§Ù†Ø© ÙˆØ§Ù„Ø·Ø±ÙŠÙ‚ ÙˆØ§Ø¶Ø­."`;
  }
  
  // Default English fallback
  return `You are Nadar, an AI assistant for blind users. Analyze the image and audio question to provide helpful guidance in a single paragraph.`;
}

function generateFollowUpSuggestions(language: string): string[] {
  if (language === 'darija') {
    return [
      'Ù†Ù‚Ø±Ø§ Ø§Ù„Ù†Øµ ÙƒØ§Ù…Ù„ØŸ',
      'ÙÙŠÙ† Ø§Ù„Ù…Ù…Ø± Ø§Ù„Ø®Ø§Ù„ÙŠØŸ',
      'Ø´Ù†Ùˆ ÙƒØ§ÙŠÙ† Ø­Ø¯Ø§ÙŠØŸ'
    ];
  }
  
  return [
    'Read all text?',
    'Where is the clear path?',
    'What is next to me?'
  ];
}

async function handler(req: VercelRequest, res: VercelResponse) {
  console.log('ğŸš€ LIVE ASSIST HANDLER CALLED!');
  console.log('ğŸ” Method:', req.method);
  console.log('ğŸ” Body:', JSON.stringify(req.body, null, 2));
  console.log('ğŸ” GEMINI_API_KEY exists:', !!process.env.GEMINI_API_KEY);

  if (req.method !== 'POST') {
    console.log('âŒ Method not allowed:', req.method);
    return res.status(405).json({ error: 'Method not allowed' });
  }

  const startTime = Date.now();
  
  try {
    const {
      sessionId,
      language = 'darija',
      style = 'single_paragraph',
      image,
      audio,
      question
    }: LiveAssistRequest = req.body;

    console.log(`ğŸš€ Live assist request: ${sessionId}, lang: ${language}, has_audio: ${!!audio}, has_image: ${!!image}`);

    if (!sessionId) {
      return res.status(400).json({ error: 'sessionId is required' });
    }

    if (!image && !audio && !question) {
      return res.status(400).json({ error: 'At least one of image, audio, or question is required' });
    }

    // Use Gemini 2.5 Flash for multimodal processing
    const model = genAI.getGenerativeModel({ 
      model: 'gemini-2.5-flash',
      systemInstruction: createSystemPrompt(language, style)
    });

    // Build the content array for multimodal input
    const content: any[] = [];

    // Add image if provided
    if (image) {
      content.push({
        inlineData: {
          data: image.data,
          mimeType: image.mime
        }
      });
    }

    // Add audio if provided
    if (audio) {
      content.push({
        inlineData: {
          data: audio.data,
          mimeType: audio.mime
        }
      });
    }

    // Add text question if provided
    if (question) {
      content.push({
        text: question
      });
    }

    // If no explicit question but we have audio/image, add a general prompt
    if (!question && (audio || image)) {
      const defaultPrompt = language === 'darija' 
        ? 'ÙˆØµÙ Ù„ÙŠ Ø´Ù†Ùˆ ÙƒØ§ÙŠÙ† ÙÙ‡Ø§Ø¯ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ¬Ø§ÙˆØ¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¯ÙŠØ§Ù„ÙŠ'
        : 'Describe what you see in this image and answer my question';
      content.push({
        text: defaultPrompt
      });
    }

    console.log(`ğŸ§  Sending multimodal request to Gemini 2.5 Flash with ${content.length} parts`);

    // Generate response
    const result = await model.generateContent(content);
    const response = await result.response;
    const text = response.text();

    const modelMs = Date.now() - startTime;
    const audioBytes = audio ? Buffer.from(audio.data, 'base64').length : 0;

    console.log(`âœ… Live assist response generated in ${modelMs}ms, audio_bytes: ${audioBytes}`);

    const liveResponse: LiveAssistResponse = {
      sessionId,
      speak: text,
      suggest: generateFollowUpSuggestions(language),
      audio_bytes: audioBytes,
      model_ms: modelMs,
      assist_engine: 'gemini-multimodal'
    };

    // Add token usage if available
    if (response.usageMetadata) {
      liveResponse.tokens_in = response.usageMetadata.promptTokenCount;
      liveResponse.tokens_out = response.usageMetadata.candidatesTokenCount;
    }

    res.status(200).json(liveResponse);

  } catch (error: any) {
    console.error('âŒ Live assist error:', error);
    
    const errorResponse = {
      error: 'Live assist failed',
      message: error?.message || 'Unknown error',
      assist_engine: 'gemini-multimodal'
    };

    res.status(500).json(errorResponse);
  }
}

// Export as default for Vercel API routes
export default handler;
